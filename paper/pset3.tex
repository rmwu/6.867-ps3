%\documentclass{amsart}

\documentclass{article}
\usepackage[letterpaper,hmargin=15mm,vmargin=20mm]{geometry}
\usepackage[nosetup, colorlinks]{tony}
\usepackage{graphicx}

\usepackage{amsmath,amssymb}
\usepackage{siunitx}

\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{multicol}

\usepackage{diagbox}

\usepackage{color}
\usepackage[dvipsnames]{xcolor}
%\usepackage[printwatermark]{xwatermark}
%\newwatermark*[allpages,color=gray!50,angle=45,scale=3,xpos=0,ypos=0]{DRAFT}

\usepackage{tikz}
\usetikzlibrary{arrows}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\NLL}{NLL}
\newcommand{\sind}[1]{^{(#1)}}

\title{6.867: Problem Set 3}
\date{November 10, 2016}

\begin{document}
\maketitle

\begin{multicols}{2}

% % % % % % % % % %
%    PROBLEM 1
% % % % % % % % % %

\section{Neural Networks}
\label{sec:nn}

\def\layersep{2.5cm}

% tikz taken and adjusted from
% http://www.texample.net/tikz/examples/neural-network/
\begin{figure*}[t]
    \label{fig:samplenn}
    \centering
    \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
        \tikzstyle{every pin edge}=[<-,shorten <=1pt]
        \tikzstyle{neuron}=[circle,fill=black!25,minimum size=16pt,inner sep=0pt]
        \tikzstyle{input neuron}=[neuron, fill=CornflowerBlue!50];
        \tikzstyle{output neuron}=[neuron, fill=Dandelion!50];
        \tikzstyle{hidden neuron}=[neuron, fill=YellowGreen!50];
        \tikzstyle{annot} = [text width=5em, text centered]
    
        % Draw the input layer nodes
        \foreach \name / \y in {1,...,2}
        % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
            \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-1cm-\y cm) {};
    
        % Draw the hidden layer nodes
        \foreach \name / \y in {1,...,5}
            \path[yshift=0.5cm]
                node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
                
        \foreach \name / \y in {6,...,10}
            \path[yshift=0.5cm]
                node[hidden neuron] (H-\name) at (2*\layersep,-\y cm + 5cm) {};
    
        % Draw the output layer node
        \foreach \name / \y in {1,...,3}
            \node[output neuron,pin={[pin edge={->}]right:Output \#\y}, right of=H-8] (O-\name) at (2*\layersep, -\y cm-0.5cm){};
    
        % Connect every node in the input layer with every node in the
        % hidden layer.
        \foreach \source in {1,...,2}
            \foreach \dest in {1,...,5}
                \path (I-\source) edge (H-\dest);
        
        % Connect every node in hidden layer 1 to hidden layer 2
        \foreach \source in {1,...,5}
            \foreach \dest in {6,...,10}
                \path (H-\source) edge (H-\dest);
    
        % Connect every node in the hidden layer with the output layer
        \foreach \source in {6,...,10}
            \foreach \dest in {1,...,3}
                \path (H-\source) edge (O-\dest);
    
        % Annotate the layers
        \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer (ReLU)};
        \node[annot,left of=hl] {Input layer};
        \node[annot,right of=hl] (hl2) {Hidden layer (ReLU)};
        \node[annot,right of=hl2] {Output layer (Softmax)};
    \end{tikzpicture}
    \caption{Topology of a fully-connected neural network, with 2D inputs and 3-class outputs.}
\end{figure*}

We implemented a traditional, fully-connected neural network, with
number of hidden layers and nodes per layer specified by the user.
We trained the neural network through stochastic gradient descent on
our data, where the stochastic gradients are calculated by the
backpropogation algorithm.

All hidden units were implemented as ReLU units, whose activation functions
are defined as
\begin{equation}
    f(z) = \max(0, z).
\end{equation}
The output layer was implemented as a Softmax layer for $k$ classes,
which is intuitively similar to a multi-dimensional sigmoid. Its activation function
is given as the vector $f(z)$ where
\begin{equation}
    f(z)_i = \f{e^{z_i}}{\sum_{j=1}^{k}{e^{z_j}}} \tx{ for } i = 1,\dots,k.
\end{equation}
We took target vectors of ``one-hot'' (1-of-k) vectors, whose loss we measured
with cross-entropy. For a given training point $(x,y)$, the loss is
\begin{equation}
l = \sum_{i = 1}^k{y_i \log(f(z)_i)}
\end{equation}
where $f(z)$ is defined above as the Softmax output.
In the backpropogation algorithm, we calculated the output layer loss using the
cross-entropy of the Softmax function,
\begin{equation}\delta^L = f'(z^L)\circ\nabla_{a^L}l = f(z^L) - y\end{equation}
where $\circ$ is element-wise multiplication, $f(z^L)$ is the Softmax output,
and $y$ is the target vector. This cross-entropy loss is propogated through the 
loss calculations for the hidden layers, which utilize the ReLU gradient instead.

For a toy problem, we tested our neural network on a 2D 3-class dataset, partitioned
into 400 training, 200 validation, and 200 testing samples.
A possible topology for this neural network is drawn in Figure \ref{fig:samplenn}.
We noticed that neural network  performance was very sensitive to 
variations in weight initialization and step size.
For example, when we tried a decaying step size of $1/t$, the neural network
converged to predicting only a single value for every given input.

\subsection{Initialization}

Since ReLU is a not fully differentiable, we set the subgradient at 0 to 0.
As a result, we cannot simply initialize weights to 0, else gradients and activations 
will not be updated during training. Instead, we initialize weights randomly,
with values drawn from a Gaussian centered at 0, with standard deviation $1/\sqrt{m}$
for a weight matrix of size $m$ by $n$.

To understand the rationale behind this method, imagine that all units are sigmoids,
with activation function $f(z) = \f{1}{1+e^{-z}}$. Then at 0, the sigmoid is approximately
linear, at which we lose the non-linearity benefit of multiple layers, but at larger values,
our activations will saturate and increase indefinitely.

Thus, we want to maintain the variance of output to inputs, such that 

then this initialization method is reasonable
since at 0, the sigmoid function  

\subsection{Regularization}

To avoid overfitting, we can add a penalty term to our loss function for regularization.
Our loss would thus become
\begin{equation}
    J(w) = l(w) +
      \lambda(\lVert w^{(1)} \rVert^2_F + \lVert w^{(2)} \rVert^2_F)
\end{equation}
where $\lVert w\rVert = \sqrt{\sum_{i,j}{w_{ij}^2}}$ is the Frobenius norm.
In particular, the regularization term is added to the output error $\delta^L$
during backpropogation.
This helps prevent weights from becoming too large, so the neural network will
overfit less.

\subsection{Binary classification}
\label{subsec:binary}

\begin{figure*}
   \centering
   \includegraphics[width=2.5in]{img/p1/4-1small-382of400-16500.pdf}
   \includegraphics[width=2.5in]{img/p1/4-2small-380of400-3550.pdf}
   \includegraphics[width=2.5in]{img/p1/4-1large-380of400.pdf}
   \includegraphics[width=2.5in]{img/p1/4-2large-379of400-123000.pdf}
   \caption{Variations on neural network architectures for classification of
   XOR data. We see that increasing number of layers increases non-linearity,
   while increasing number of nodes per layer increases overfitting. 
   }
   \label{fig:1-2-arch}
\end{figure*}

We tested our implementation against 2D artificial datasets.
The first three contained 400 training samples,
200 validation samples,
and 200 testing samples, and fourth contained 
400 training, validation, and testing samples.
The data consisted of two-dimensional vectors $x\sind{i}$ labeled by $y\sind{i} = \pm1$.
To compare the target values against our Softmax outputs, we mapped $\pm1$ to
one-hot vectors $[1,0]$ and $[0,1]$.

We trained our neural network with stochastic gradient descent, with a 
fixed stepsize, $\eta = 0.01$. Stochastic gradient descent terminated when
the change in cross-entropy loss on our validation set dropped below a constant threshold of 0.001.
This loss was calculated once every 100 epochs.

% TODO actually get reliable results from this
% TODO nice images

\subsubsection{Architecture variations}
\label{subsubsec:binaryarch}
We investigated different architectures for the neural network, including
\begin{enumerate}
\item 1 small hidden layer (5 nodes)
\item 1 large hidden layer (100 nodes)
\item 2 small hidden layers (5 nodes each)
\item 2 large hidden layers (100 nodes each)
\end{enumerate}

We compared our neural network's performance to that of a kernelized
Gaussian RBG SVM classifier and observed similar performance in accuracy.

\subsection{MNIST multi-class classification}

We also used our neural network to perform multi-class classification of the
MNIST dataset. These data consist $28\times 28$ grayscale images of handwritten
digits, 0 to 9, given as a labeled dataset of $28^2$ dimensional vectors
with integer entries $[0,255]$ representing the grayscale luminescence of each pixel.

We normalized input pixel features to the range $[-1, 1]$,
so that an input vector $x$ is mapped to $\f{2x}{255}-1$,
with all operations applied componentwise. Normalization significantly
improved prediction accuracy. Class labels were mapped to one-hot vector 
representations. For example, an image labeled 9 would have a 
target vector of $[0,0,0,0,0,0,0,0,0,1]$.
We selected 400 training, 200 validation, and 200 testing samples per class.

As in Section \ref{subsec:binary}, we determined the termination criteria as
when the classification accuracy on the validation set exceeded 0.99,
or when the change in objective function (cross-entropy loss on Softmax output)
dropped below a constant threshold, 0.001 (checked every 200 epochs).

\subsubsection{Architecture variations}

We classified our data with the same architectures enumerated in Section \ref{subsubsec:binaryarch}.

% % % % % % % % % %
%    PROBLEM 2
% % % % % % % % % %

\section{Convolutional Neural Networks}

% TODO connect from previous section

Empirically, deeper convolution networks work better.
To see why, we introduce the notion of \emph{receptive field}
of a node (a ``pixel" in a feature map):
the pixels from the original image that contribute information
to the node.

% What are the dimensions of the receptive field for a node in Z2?
For example,
suppose we have a convolutional network
with two successive convolutional layers,
with respective patch sizes $5\times 5$ and $3\times 3$.
Each layer has a single feature map (which we name $Z_1$ and $Z_2$).
Then the nodes in $Z_1$ each have $5\times 5$ receptive fields.
Nodes in $Z_2$ will have $(3 + 5 - 1)^2 = 7\times 7$ receptive fields.

% Thinking about your answer,
% why is it effective to build convolutional networks deeper
% (i.e. with more layers)?
In general,
nodes in deeper layers will have larger receptive fields,
so intuitively,
deeper convolutional networks should better detect large-scale structure.

% How many layers are there?
We tested an existing four-layer convolutional network implementation
on an image classification problem,
where given RGB images of 451 paintings by 11 artists
(all downsampled to $50\times 50$),
we would like to predict the artist that painted the given image.

% Are they all convolutional?
% If not, what structure do they have?
The first two layers are convolutional;
each produces 16 feature maps
with $5\times 5$ filters and stride $2$.
We zero-pad the feature maps prior to convolution
so that feature maps remain the same size ($50\times 50$).

Our last two layers are fully connected.
The third layer has 64 hidden units;
the final layer (our output) has 11 units,
corresponding to the 11 artists represented in our dataset.

% Which activation function is used on the hidden nodes?
The network applies a ReLU activation on all hidden nodes.

% What loss function is being used to train the network?
% How is the loss being minimized?
We train the network by minimizing softmax cross-entropy
through minibatch gradient descent,
with batches of 10 examples each.
Recall that average softmax cross-entropy is defined
for labeled examples $(x\sind{i}, y\sind{i})$
(where $y\sind{i}$ is a one-hot vector indicating the category of the example)
as
\begin{equation}
    H(y, \hat y) = \f{1}{n}\sum_{i=1}^n
                             \sum_{j=1}^k
                               y\sind{i}_j \log \hat y\sind{i}_j,
\end{equation}
where $\hat y\sind{i}$ is the prediction made by our network,
a vector of probabilities
produced by applying a softmax function
to the values on our output layer.
For instance, for our image classification problem,
$n$ is the number of images we are evaluating our network on,
and $k=11$ is the number of classification categories.

% What is the training accuracy for your network after training?
% What is the validation accuracy?
% What do these two numbers tell you about what your network is doing?
After 1500 training steps with step size $\eta = 0.01$,
our convolutional network obtains perfect training accuracy,
but only $67\%$ validation accuracy,
strongly suggesting that our network is overfitting.

\subsection{Pooling}

% and choose different values for the pooling filter size and stride.
% After you applied max pooling, what happened to your results?
% How did the training accuracy vs. validation accuracy change?
We modified our network by adding max-pooling
after both convolutional layers.
We experimented with various pooling filter sizes (1 to 4)
and strides (1 to 4).
In all cases,
pooling modestly reduced training accuracy
(from around $100\%$ to $90\%$),
but did not improve validation accuracy noticeably.
Accuracy was typically $60\%$ to $70\%$.

Variations in validation accuracy
stemmed from the randomness inherent in our stochastic training scheme,
as variation in accuracy between runs with different hyperparameters
was comparable to the that between runs with the same hyperparameters.
% make less verbose?

% What does that tell you about the effect of max pooling on your network?
We conclude that max-pooling provides little improvement
to our convolutional network's accuracy.
We do note, however, that overfitting was reduced,
since training accuracy matched validation accuracy more closely.

\subsection{Regularization}

To combat the overfitting we saw earlier,
we tested various methods of regularization.
Unless otherwise stated,
in each section we \emph{only} vary the hyperparameter in question;
we leave the remaining hyperparameters (e.g. training steps, pooling) unchanged.

% Test each one individually,
% and discuss how it affects your results.

\subsubsection{Dropout}


We implemented dropout for the fully connected layer (the third layer).
We tested various values of dropout parameter~$p$,
the probability we do \emph{not} drop out a number.
For each $p$, we trained $n=10$ separate models;
we present the mean training and validation accuracies
with their respective standard errors here.
Training accuracies are quoted on the final minibatch of ten samples.
\begin{center}
    \begin{tabular}{c|cc}
        $p$ & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        0.6 &  $77.00 \pm 2.60$ & $62.18 \pm 0.81$ \\
        0.7 &  $75.00 \pm 3.42$ & $62.53 \pm 0.86$ \\
        0.8 &  $89.00 \pm 3.48$ & $65.52 \pm 1.24$ \\
        0.9 &  $94.00 \pm 2.21$ & $65.63 \pm 0.55$ \\
        1.0 & $100.00 \pm 0.00$ & $65.63 \pm 1.01$ \\
    \end{tabular}
\end{center}

% You should explore different values of dropout_prob
% to find one that works well.
While dropout reduces training accuracy significantly,
it only modestly affects validation accuracy.
So while dropout combats overfitting,
it doesn't improve our model's ability to generalize.

We also tested dropout together with pooling;
validation accuracies were slightly, but noticeably lower.


\subsubsection{Weight regularization}

As we did previously,  % TODO check that this is mentioned in previous section
we experimented with adding a regularization term
to our training loss.
In particular, we modify our softmax cross-entropy loss $\ell(w)$
to get a new loss
\begin{equation}
    \ell(w) + \lambda\sum_i \lVert w\sind{i} \rVert^2
\end{equation}
where we include an extra term:
the sum of the squared Frobenius norms of the weight matrices $w\sind{i}$
of the two fully-connected layers,
weighted by a regularization parameter~$\lambda$.

Again, we test each value of $\lambda$ ten times,
and present the mean accuracies (with associated standard errors) below.

\begin{center}
    \begin{tabular}{c|cc}
        $\lambda$ & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        0    & $100.00 \pm 0.00$ & $65.40 \pm 1.19$ \\
        0.01 & $100.00 \pm 0.00$ & $66.44 \pm 1.15$ \\
        0.03 & $100.00 \pm 0.00$ & $66.55 \pm 0.76$ \\
        0.1  & $100.00 \pm 0.00$ & $65.40 \pm 0.81$ \\
        0.3  &  $79.00 \pm 2.33$ & $52.64 \pm 1.36$ \\
      % 1.0  &  $50.00 \pm 0.00$ & $37.70 \pm 0.33$ \\
    \end{tabular}
\end{center}
Small values of $\lambda$ produced
statistically insignificant improvement in validation accuracies;
larger values saw significantly worse performance
in both training and validation accuracies.
Thus, weight regularization
is unlikely to be particularly helpful.


\subsubsection{Data augmentation}

We next enlarged our training dataset
with transformed versions of the original training samples
and trained on this augmented dataset.
This new dataset was four times as large as the original,
so we quadrupled the number of training steps
when working with the augmented training data.

We present our results as below.
As before, we repeat training ten times
and report means and standard errors.
\begin{center}
    \begin{tabular}{cc|cc}
        Augmented & Steps & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        No  & 1500 & $100.00 \pm 0.00$ & $65.98 \pm 1.03$ \\
%        No  & 6000 & $100.00 \pm 0.00$ & $66.21 \pm 0.86$ \\
        Yes & 6000 & $100.00 \pm 0.00$ & $62.30 \pm 0.61$
    \end{tabular}
\end{center}

Note that data augmentation actually lowered our validation accuracy.
Indeed, using the augmented data with 6000 steps
produced statistically significantly worse results
than using the original training set with 1500 steps
(Student's $t$-test, $p=0.0066$).


%    Finished training. Total time taken: 741.599348783493
%    Validation accuracies: [0.6091954,0.63218391,0.63218391,0.6091954,0.64367816,0.59770115,0.65517241,0.63218391,0.62068966,0.59770115]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 62.30% +- 0.61%
%    You ran command: conv.py --repeat 10 --augmented --training_steps 6001

%    Finished training. Total time taken: 591.5699789524078
%    Validation accuracies: [ 0.68965517,0.68965517,0.66666667,0.65517241,0.68965517,0.6091954,0.64367816,0.66666667,0.67816092,0.63218391]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 66.21% +- 0.86%
%    You ran command: conv.py --repeat 10 --training_steps 6001

%    Finished training. Total time taken: 151.98044800758362
%    Validation accuracies: [0.67816092,0.62068966,0.65517241,0.71264368,0.71264368,0.65517241,0.65517241,0.64367816,0.62068966,0.64367816]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 65.98% +- 1.03%
%    You ran command: conv.py --repeat 10 --training_steps 1501


\subsubsection{Early stopping}

Finally, we consider early stopping as a means of regularization.
We monitored the validation error
of our model each time we trained it;
we show a representative plot in Figure~\ref{fig:2-4-4-validation-acc}.

\begin{figure*}[t]
   \centering
   \includegraphics[width=4in]{img/2-4-4-validation-acc-new.pdf}
   \caption{Validation error of our convolutional network during training.}
   \label{fig:2-4-4-validation-acc}
\end{figure*}

After around 1000 training steps,
validation accuracy fails to improve further.
We note, however, that validation accuracy
does not significantly \emph{decrease}
as we continue to train our model,
suggesting that overfitting (if it is occurring at all)
is not negatively impacting our results.

In summary, none of our regularization schemes
was particularly effective in improving validation accuracy.



\subsection{Architectural variations}

We also experimented with varying our network architecture,
rather than just with training parameters
(e.g. weight regularization, dropout).

%    Report which changes led to
%    the biggest increases and decreases in performance.
%    In particular,
%    what is the effect of making the convolutional layers have
%    a) a larger filter size,

\subsubsection{Filter sizes}

We tested larger and smaller filter sizes
on each convolutional layer.
We first tried changing the filter sizes on both layers simultaneously,
leaving other hyperparameters fixed.
Our results show that filters larger than $5\times 5$
work just as well,
while smaller filters perform significantly worse.

We also tried more novel filter size combinations.
None yielded any statistically significant improvement
over our original $5\times 5$ filters.

We present our findings
in Figure~\ref{fig:filter-size-experiment}.

\begin{figure*}[t]
   \centering
   \includegraphics[width=4in]{img/2-5-filter-size-acc.pdf} 
   \caption{Mean validation accuracies for various filter size choices (each tested $n=10$ times).
       Error bars denote standard error of the mean.}
   \label{fig:filter-size-experiment}
\end{figure*}

%\begin{center}
%    \begin{tabular}{c|cc}
%        Filter sizes & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
%        $3\times 3$ & $95.00 \pm 1.67$ & $60.69 \pm 0.98$ \\
%        $5\times 5$ & $100.00 \pm 0.00$ & $65.98 \pm 1.03$ \\
%        $7\times 7$ & $100.00 \pm 0.00$ & $64.02 \pm 1.07$ \\
%        $3^2$ then $9^2$ & $100.00 \pm 0.00$ & $66.44 \pm 1.12$ \\
%        $5^2$ then $3^2$ & $98.00 \pm 2.00$ & $64.60 \pm 0.61$ \\
%        $9^2$ then $3^2$ & $100.00 \pm 0.00$ & $65.06 \pm 0.88$ \\        
%        $9^2$ then $5^2$ & $100.00 \pm 0.00$ & $65.52 \pm 0.62$
%    \end{tabular}
%\end{center}


\subsubsection{Stride}

%    b) a larger stride and
We tried varying the strides on our convolutional layers as well.
No setting we tried yielded statistically significant improvement
over our baseline $5\times 5$ filters with stride $2$.


%    c) greater depth?
\subsubsection{Depth}

Finally, we considered the number of feature maps
we used at each layer.



%    How does a pyramidal-shaped network
%    in which the feature maps gradually decrease in height and width
%    but increase in depth
%    compare to a flat architecture,
%    or one with the opposite shape?




%    Finished training. Total time taken: 185.3328869342804
%    Training accuracies: [ 0.9  0.9  0.9  0.9  0.9  0.8  0.8  0.9  1.   1. ]
%    Validation accuracies: [ 0.65517241  0.64367816  0.67816092  0.64367816  0.71264368  0.66666667
%      0.64367816  0.71264368  0.68965517  0.65517241]
%    Mean training accuracy: 90.00% +- 2.11%
%    Mean validation accuracy: 67.01% +- 0.86%
%    You ran command: conv.py --pooling --depths 12 24 --repeat 10

%    Finished training. Total time taken: 222.4871289730072
%    Training accuracies: [ 1.   0.9  0.9  0.9  1.   1.   0.8  1.   0.8  1. ]
%    Validation accuracies: [ 0.67816092  0.70114943  0.66666667  0.65517241  0.67816092  0.64367816
%      0.63218391  0.71264368  0.70114943  0.66666667]
%    Mean training accuracy: 93.00% +- 2.60%
%    Mean validation accuracy: 67.36% +- 0.83%
%    You ran command: conv.py --pooling --depths 24 12 --repeat 10

%    Finished training. Total time taken: 203.84384393692017
%    Training accuracies: [ 0.9  1.   1.   0.9  0.9  1.   0.9  1.   1.   1. ]
%    Validation accuracies: [ 0.67816092  0.64367816  0.68965517  0.67816092  0.66666667  0.67816092
%      0.64367816  0.67816092  0.59770115  0.67816092]
%    Mean training accuracy: 96.00% +- 1.63%
%    Mean validation accuracy: 66.32% +- 0.87%
%    You ran command: conv.py --pooling --depths 10 30 --repeat 10




\end{multicols}

\end{document}



%    Finished training. Total time taken: 174.98675990104675
%    Training accuracies: [ 1.   1.   1.   1.   1.   0.8  1.   1.   0.9  1. ]
%    Validation accuracies: [ 0.75862069  0.73563218  0.65517241  0.68965517  0.68965517  0.68965517
%      0.66666667  0.64367816  0.6091954   0.66666667]
%    Mean training accuracy: 97.00% +- 2.13%
%    Mean validation accuracy: 68.05% +- 1.37%
%    You ran command: conv.py --filter_sizes 9 5 --pooling --repeat 10

%    Finished training. Total time taken: 244.02631878852844
%    Training accuracies: [ 0.9  1.   0.9  1.   0.9  1.   1.   1.   1.   1. ]
%    Validation accuracies: [ 0.67816092  0.62068966  0.63218391  0.72413793  0.68965517  0.68965517
%      0.64367816  0.67816092  0.62068966  0.72413793]
%    Mean training accuracy: 97.00% +- 1.53%
%    Mean validation accuracy: 67.01% +- 1.24%

%    67.5287356 \pm 0.905346418252
%    You ran command: conv.py --filter_sizes 9 5 --pooling --repeat 10

%    Finished training. Total time taken: 130.73607516288757
%    Training accuracies: [ 0.9  1.   1.   1.   0.9  1.   0.9  1.   1.   0.9]
%    Validation accuracies: [ 0.67816092  0.64367816  0.63218391  0.64367816  0.62068966  0.62068966
%      0.66666667  0.65517241  0.6091954   0.66666667]
%    Mean training accuracy: 96.00% +- 1.63%
%    Mean validation accuracy: 64.37% +- 0.73%
%    You ran command: conv.py --filter_sizes 7 5 --pooling --repeat 10

%    Finished training. Total time taken: 191.86493802070618
%    Training accuracies: [ 1.   0.9  1.   0.9  0.7  0.6  1.   1.   0.8  1. ]
%    Validation accuracies: [ 0.6091954   0.71264368  0.6091954   0.68965517  0.64367816  0.64367816
%      0.68965517  0.66666667  0.65517241  0.64367816]
%    Mean training accuracy: 89.00% +- 4.58%
%    Mean validation accuracy: 65.63% +- 1.08%
%    You ran command: conv.py --filter_sizes 9 3 --pooling --repeat 10




%    Finished training. Total time taken: 89.55591106414795
%    Training accuracies: [ 0.9  0.8  0.9  0.8  0.9  0.9  0.8  0.8  0.7  0.9]
%    Validation accuracies: [ 0.66666667  0.64367816  0.68965517  0.67816092  0.63218391  0.6091954
%      0.6091954   0.63218391  0.5862069   0.5862069 ]
%    Mean training accuracy: 84.00% +- 2.21%
%    Mean validation accuracy: 63.33% +- 1.16%
%    You ran command: conv.py --strides 4 4 --repeat 10

%    Finished training. Total time taken: 1270.8897230625153
%    Training accuracies: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
%    Validation accuracies: [ 0.63218391  0.59770115  0.5862069   0.62068966  0.64367816  0.64367816
%      0.59770115  0.64367816  0.52873563  0.51724138]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 60.11% +- 1.46%
%    You ran command: conv.py --strides 1 1 --repeat 10

%    Validation accuracy: 64.37%
%    Finished training. Total time taken: 134.22611594200134
%    Training accuracies: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
%    Validation accuracies: [ 0.62068966  0.65517241  0.64367816  0.62068966  0.62068966  0.64367816
%      0.67816092  0.70114943  0.59770115  0.64367816]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 64.25% +- 0.96%
%    You ran command: conv.py --strides 3 1 --repeat 10


