%\documentclass{amsart}

\documentclass{article}
\usepackage[letterpaper,hmargin=15mm,vmargin=20mm]{geometry}
\usepackage[nosetup, colorlinks]{tony}
\usepackage{graphicx}

\usepackage{amsmath,amssymb}
\usepackage{siunitx}

\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{multicol}

\usepackage{diagbox}

\usepackage{xcolor}
%\usepackage[printwatermark]{xwatermark}
%\newwatermark*[allpages,color=gray!50,angle=45,scale=3,xpos=0,ypos=0]{DRAFT}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\NLL}{NLL}
\newcommand{\sind}[1]{^{(#1)}}

\title{6.867: Problem Set 3}
\date{November 10, 2016}

\begin{document}
\maketitle

\begin{multicols}{2}

% % % % % % % % % %
%    PROBLEM 1
% % % % % % % % % %

\section{Neural Networks}
\label{sec:nn}

We implemented a traditional, fully-connected neural network, with
number of hidden layers and nodes per layer specified by the user.
We trained the neural network through stochastic gradient descent on
our data, where the stochastic gradients are calculated by the
backpropogation algorithm.

All hidden units were implemented as ReLU units, whose activation functions
are defined as
\begin{equation}f(z) = \max(0, z).\end{equation}
The output layer was implemented as a Softmax layer for $k$ classes,
which is intuitively similar to a multi-dimensional sigmoid. Its activation function
is given as the vector $f(z)$ where
\begin{equation}
f(z)_i = \f{e^{z_i}}{\sum_{j=1}^{k}{e^{z_j}}} \tx{ for } i = 1,\dots,k.
\end{equation}
We took target vectors of ``one-hot'' (1-of-k) vectors, whose loss we measured
with cross-entropy. For a given training point $(x,y)$, the loss is
\begin{equation}
l = \sum_{i = 1}^k{y_i \log(f(z)_i)}
\end{equation}
where $f(z)$ is defined above as the Softmax output.
In the backpropogation algorithm, we calculated the output layer loss using the
cross-entropy of the Softmax function,
\begin{equation}\delta^L = f'(z^L)\circ\nabla_{a^L}l = f(z^L) - y\end{equation}
where $\circ$ is element-wise multiplication, $f(z^L)$ is the Softmax output,
and $y$ is the target vector. This cross-entropy loss is propogated through the 
loss calculations for the hidden layers, which utilize the ReLU gradient instead.

\subsection{Initialization}

Since ReLU is a not fully differentiable, we set the subgradient at 0 to 0.
As a result, we cannot simply initialize weights to 0, else all gradients and activations 
will not be updated during training. Instead, we initialize weights randomly,
with values drawn from a Gaussian centered at 0, with standard deviation $1/\sqrt{m}$
for a weight matrix of size $m$ by $n$.

If all units were sigmoids, rather than ReLUs, then this initialization method would 
be reasonable since... % TODO

\subsection{Regularization}

To avoid overfitting, we can add a penalty term to our loss function for regularization.
Our loss would thus become
\begin{equation}J(w) = l(w) + \lambda(||w^{(1)}||^2_F + ||w^{(2)}||^2_F)
\end{equation}
where $||w|| = \sqrt{\sum_{i,j}{w_{ij}^2}}$ is the Frobenius norm.

\subsection{Binary classification}

We tested our implementation against 2D artificial datasets.
The first three contained 400 training samples,
200 validation samples,
and 200 testing samples, and fourth contained 
400 training, validation, and testing samples.
The data consisted of two-dimensional vectors $x\sind{i}$ labeled by $y\sind{i} = \pm1$.
To compare the target values against our Softmax outputs, we mapped $\pm1$ to
one-hot vectors $[1,0]$ and $[0,1]$.

We trained our neural network with stochastic gradient descent, with a 
decaying stepsize, $\eta = 1/t$. Stochastic gradient descent terminated when
the cross-entropy loss on our validation set dropped below a constant threshold.
This loss was calculated once every 100 epochs.

% TODO actually get reliable results from this
% TODO nice images

\subsubsection{Architecture variations}

We investigated different architectures for the neural network, including
\begin{enumerate}
\item 1 small hidden layer (5 nodes)
\item 1 large hidden layer (20 nodes)
\item 2 small hidden layers (5 nodes each)
\item 2 large hidden layers (20 nodes each)
\end{enumerate}

We compared our neural network's performance to that of a kernelized
Gaussian RBG SVM classifier and observed similar performance in accuracy.

\subsection{MNIST multi-class classification}

We also used our neural network to perform multi-class classification of the
MNIST dataset. These data consist $28\times 28$ grayscale images of handwritten
digits, 0 to 9, given as a labeled dataset of $28^2$ dimensional vectors
with integer entries $[0,255]$ representing the grayscale luminescence of each pixel.

We normalized input pixel features to the range $[-1, 1]$,
so that an input vector $x$ is mapped to $\f{2x}{255}-1$,
with all operations applied componentwise. Normalization significantly
improved prediction accuracy. Class labels were mapped to one-hot vector 
representations. For example, an image labeled as 9 would have a 
target vector of $[0,0,0,0,0,0,0,0,0,1]$.
We selected 300 training, 150 validation, and 150 testing samples per class.

% % % % % % % % % %
%    PROBLEM 2
% % % % % % % % % %

\section{Convolutional Neural Networks}

% TODO connect from previous section

Empirically, deeper convolution networks work better.
To see why, we introduce the notion of \emph{receptive field}
of a node (a ``pixel" in a feature map):
the pixels from the original image that contribute information
to the node.

% What are the dimensions of the receptive field for a node in Z2?
For example,
suppose we have a convolutional network
with two successive convolutional layers,
with respective patch sizes $5\times 5$ and $3\times 3$.
Each layer has a single feature map (which we name $Z_1$ and $Z_2$).
Then the nodes in $Z_1$ each have $5\times 5$ receptive fields.
Nodes in $Z_2$ will have $(3 + 5 - 1)^2 = 7\times 7$ receptive fields.

% Thinking about your answer,
% why is it effective to build convolutional networks deeper
% (i.e. with more layers)?
In general,
nodes in deeper layers will have larger receptive fields,
so intuitively,
deeper convolutional networks should better detect large-scale structure.

% How many layers are there?
We tested an existing four-layer convolutional network implementation
on an image classification problem,
where given RGB images of 451 paintings by 11 artists
(all downsampled to $50\times 50$),
we would like to predict the artist that painted the given image.

% Are they all convolutional?
% If not, what structure do they have?
The first two layers are convolutional;
each produces 16 feature maps
with $5\times 5$ filters and stride $2$.
We zero-pad the feature maps prior to convolution
so that feature maps remain the same size ($50\times 50$).

Our last two layers are fully connected.
The third layer has 64 hidden units;
the final layer (our output) has 11 units,
corresponding to the 11 artists represented in our dataset.

% Which activation function is used on the hidden nodes?
The network applies a ReLU activation on all hidden nodes.

% What loss function is being used to train the network?
% How is the loss being minimized?
We train the network by minimizing softmax cross-entropy
through minibatch gradient descent,
with batches of 10 examples each.
Recall that average softmax cross-entropy is defined
for labeled examples $(x\sind{i}, y\sind{i})$
(where $y\sind{i}$ is a one-hot vector indicating the category of the example)
as
\begin{equation}
    H(y, \hat y) = \f{1}{n}\sum_{i=1}^n
                             \sum_{j=1}^k
                               y\sind{i}_j \log \hat y\sind{i}_j,
\end{equation}
where $\hat y\sind{i}$ is the prediction made by our network,
a vector of probabilities
produced by applying a softmax function
to the values on our output layer.
For instance, for our image classification problem,
$n$ is the number of images we are evaluating our network on,
and $k=11$ is the number of classification categories.

% What is the training accuracy for your network after training?
% What is the validation accuracy?
% What do these two numbers tell you about what your network is doing?
After 1500 training steps with step size $\eta = 0.01$,
our convolutional network obtains perfect training accuracy,
but only $67\%$ validation accuracy,
strongly suggesting that our network is overfitting.

\subsection{Pooling}

% and choose different values for the pooling filter size and stride.
% After you applied max pooling, what happened to your results?
% How did the training accuracy vs. validation accuracy change?
We modified our network by adding max-pooling
after both convolutional layers.
We experimented with various pooling filter sizes (1 to 4)
and strides (1 to 4).
In all cases,
pooling modestly reduced training accuracy
(from around $100\%$ to $90\%$),
but did not improve validation accuracy noticeably.
Accuracy was typically $60\%$ to $70\%$.

Variations in validation accuracy
stemmed from the randomness inherent in our stochastic training scheme,
as variation in accuracy between runs with different hyperparameters
was comparable to the that between runs with the same hyperparameters.
% make less verbose?

% What does that tell you about the effect of max pooling on your network?
We conclude that max-pooling provides little improvement
to our convolutional network's accuracy.
We do note, however, that overfitting was reduced,
since training accuracy matched validation accuracy more closely.

\subsection{Regularization}

To combat the overfitting we saw earlier,
we tested various methods of regularization.
Unless otherwise stated,
in each section we \emph{only} vary the hyperparameter in question;
we leave the remaining hyperparameters (e.g. training steps, pooling) unchanged.

% Test each one individually,
% and discuss how it affects your results.

\subsubsection{Dropout}


We implemented dropout for the fully connected layer (the third layer).
We tested various values of dropout parameter~$p$,
the probability we do \emph{not} drop out a number.
For each $p$, we trained $n=10$ separate models;
we present the mean training and validation accuracies
with their respective standard errors here.
Training accuracies are quoted on the final minibatch of ten samples.
\begin{center}
    \begin{tabular}{c|cc}
        $p$ & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        0.6 &  $77.00 \pm 2.60$ & $62.18 \pm 0.81$ \\
        0.7 &  $75.00 \pm 3.42$ & $62.53 \pm 0.86$ \\
        0.8 &  $89.00 \pm 3.48$ & $65.52 \pm 1.24$ \\
        0.9 &  $94.00 \pm 2.21$ & $65.63 \pm 0.55$ \\
        1.0 & $100.00 \pm 0.00$ & $65.63 \pm 1.01$ \\
    \end{tabular}
\end{center}

% You should explore different values of dropout_prob
% to find one that works well.
While dropout reduces training accuracy significantly,
it only modestly affects validation accuracy.
So while dropout combats overfitting,
it doesn't improve our model's ability to generalize.

We also tested dropout together with pooling;
validation accuracies were slightly, but noticeably lower.


\subsubsection{Weight regularization}

As we did previously,  % TODO check that this is mentioned in previous section
we experimented with adding a regularization term
to our training loss.
In particular, we modify our softmax cross-entropy loss $\ell(w)$
to get a new loss
\begin{equation}
    \ell(w) + \lambda\sum_i \lVert w\sind{i} \rVert^2
\end{equation}
where we include an extra term:
the sum of the squared Frobenius norms of the weight matrices $w\sind{i}$
of the two fully-connected layers,
weighted by a regularization parameter~$\lambda$.

Again, we test each value of $\lambda$ ten times,
and present the mean accuracies (with associated standard errors) below.

\begin{center}
    \begin{tabular}{c|cc}
        $\lambda$ & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        0    & $100.00 \pm 0.00$ & $65.40 \pm 1.19$ \\
        0.01 & $100.00 \pm 0.00$ & $66.44 \pm 1.15$ \\
        0.03 & $100.00 \pm 0.00$ & $66.55 \pm 0.76$ \\
        0.1  & $100.00 \pm 0.00$ & $65.40 \pm 0.81$ \\
        0.3  &  $79.00 \pm 2.33$ & $52.64 \pm 1.36$ \\
      % 1.0  &  $50.00 \pm 0.00$ & $37.70 \pm 0.33$ \\
    \end{tabular}
\end{center}
Small values of $\lambda$ produced
statistically insignificant improvement in validation accuracies;
larger values saw significantly worse performance
in both training and validation accuracies.
Thus, weight regularization
is unlikely to be particularly helpful.


\subsubsection{Data augmentation}

We next enlarged our training dataset
with transformed versions of the original training samples
and trained on this augmented dataset.
This new dataset was four times as large as the original,
so we quadrupled the number of training steps
when working with the augmented training data.

We present our results as below.
As before, we repeat training ten times
and report means and standard errors.
\begin{center}
    \begin{tabular}{cc|cc}
        Augmented & Steps & Train $\pm 1 \sigma$ (\%) & Validation $\pm 1 \sigma$\\\hline
        No  & 1500 & $100.00 \pm 0.00$ & $65.98 \pm 1.03$ \\
%        No  & 6000 & $100.00 \pm 0.00$ & $66.21 \pm 0.86$ \\
        Yes & 6000 & $100.00 \pm 0.00$ & $62.30 \pm 0.61$
    \end{tabular}
\end{center}

Note that data augmentation actually lowered our validation accuracy.
Indeed, using the augmented data with 6000 steps
produced statistically significantly worse results
than using the original training set with 1500 steps
(Student's $t$-test, $p=0.0066$).


%    Finished training. Total time taken: 741.599348783493
%    Validation accuracies: [0.6091954,0.63218391,0.63218391,0.6091954,0.64367816,0.59770115,0.65517241,0.63218391,0.62068966,0.59770115]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 62.30% +- 0.61%
%    You ran command: conv.py --repeat 10 --augmented --training_steps 6001

%    Finished training. Total time taken: 591.5699789524078
%    Validation accuracies: [ 0.68965517,0.68965517,0.66666667,0.65517241,0.68965517,0.6091954,0.64367816,0.66666667,0.67816092,0.63218391]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 66.21% +- 0.86%
%    You ran command: conv.py --repeat 10 --training_steps 6001

%    Finished training. Total time taken: 151.98044800758362
%    Validation accuracies: [0.67816092,0.62068966,0.65517241,0.71264368,0.71264368,0.65517241,0.65517241,0.64367816,0.62068966,0.64367816]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 65.98% +- 1.03%
%    You ran command: conv.py --repeat 10 --training_steps 1501


\subsubsection{Early stopping}

Finally, we consider early stopping as a means of regularization.
We monitored the validation error
of our model each time we trained it;
we show a representative plot in Figure~\ref{fig:2-4-4-validation-acc}.

\begin{figure*}[t]
   \centering
   \includegraphics[width=4in]{img/2-4-4-validation-acc-new.pdf}
   \caption{Validation error of our convolutional network during training.}
   \label{fig:2-4-4-validation-acc}
\end{figure*}

After around 1000 training steps,
validation accuracy fails to improve further.
We note, however, that validation accuracy
does not significantly \emph{decrease}
as we continue to train our model,
suggesting that overfitting (if it is occurring at all)
is not negatively impacting our results.

In summary, none of our regularization schemes
was particularly effective in improving validation accuracy.



\subsection{Architectural variations}

%    Finished training. Total time taken: 174.98675990104675
%    Training accuracies: [ 1.   1.   1.   1.   1.   0.8  1.   1.   0.9  1. ]
%    Validation accuracies: [ 0.75862069  0.73563218  0.65517241  0.68965517  0.68965517  0.68965517
%      0.66666667  0.64367816  0.6091954   0.66666667]
%    Mean training accuracy: 97.00% +- 2.13%
%    Mean validation accuracy: 68.05% +- 1.37%
%    You ran command: conv.py --filter_sizes 9 5 --pooling --repeat 10

%    Finished training. Total time taken: 130.73607516288757
%    Training accuracies: [ 0.9  1.   1.   1.   0.9  1.   0.9  1.   1.   0.9]
%    Validation accuracies: [ 0.67816092  0.64367816  0.63218391  0.64367816  0.62068966  0.62068966
%      0.66666667  0.65517241  0.6091954   0.66666667]
%    Mean training accuracy: 96.00% +- 1.63%
%    Mean validation accuracy: 64.37% +- 0.73%
%    You ran command: conv.py --filter_sizes 7 5 --pooling --repeat 10

%    Finished training. Total time taken: 210.48802614212036
%    Training accuracies: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
%    Validation accuracies: [ 0.65517241  0.66666667  0.71264368  0.62068966  0.67816092  0.65517241
%      0.66666667  0.64367816  0.65517241  0.65517241]
%    Mean training accuracy: 100.00% +- 0.00%
%    Mean validation accuracy: 66.09% +- 0.75%
%    You ran command: conv.py --filter_sizes 9 5 --repeat 10

\end{multicols}

\end{document}
